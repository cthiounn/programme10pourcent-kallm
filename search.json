[
  {
    "objectID": "III-Deploiements/3_Socle_Production.html",
    "href": "III-Deploiements/3_Socle_Production.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Les LLM sont des modèles de langage puissants qui nécessitent des ressources informatiques importantes pour fonctionner efficacement. Pour mettre en production une application utilisant des LLM, il est essentiel de choisir le bon matériel et les bons outils pour garantir la disponibilité des applications et des performances optimales.\nPour certains modèles de langage, les processeurs habituels appelés CPU (Central Processing Unit) peuvent suffire. Mais pour la plupart des modèles plus importants, pour que les calculs se fassent dans des temps raisonnables, il est nécessaire de se doter d’unités de traitement graphique (GPU). Nous allons donc nous intéresser ici aux différents critères à étudier pour choisir correctement des GPUs, aux outils qui permettent de suivre leurs performances et enfin le lien avec les essentiels de déploiements en termes de management de ressources (avec l’exemple du lien à Kubernetes).\n\n\nLa sélection des GPU (Graphics Processing Units) pour une installation dans une structure dépend de multiples facteurs. En voici quelques uns :\n\nPuissance de calcul : La puissance de traitement des GPU est mesurée en flops (floating-point operations per second). Un GPU plus puissant permettra d’exécuter des tâches plus rapidement et de gérer des charges de travail plus élevées.\nMémoire vive : La mémoire vive (VRAM) des GPU est essentielle pour les applications nécessitant une grande quantité de mémoire, comme les simulations scientifiques ou les applications de traitement d’image. Assurez-vous de choisir des GPU avec suffisamment de mémoire vive pour répondre aux besoins de vos applications.\nÉnergie et consommation : Les GPU consomment de l’énergie et génèrent de la chaleur. Choisissez des GPU économes en énergie et dotés de systèmes de refroidissement efficaces pour réduire les coûts énergétiques et améliorer la durée de vie des composants.\nCoût et rentabilité : Évaluez le coût total de possession (TCO) des GPU, en tenant compte des coûts d’achat, de maintenance et d’énergie. Choisissez des GPU qui offrent une bonne rentabilité pour votre administration.\nCompatibilité avec les systèmes d’exploitation : Vérifiez que les GPU sont compatibles avec les systèmes d’exploitation utilisés dans votre administration, tels que Windows, Linux ou macOS.\n\nPour le dernier point, il est commun d’acheter les GPUs par plusieurs, déjà groupés dans des serveurs. Il faut faire attention cependant au format et aux besoins spécifiques de ces serveurs, qui ne sont souvent pas standards par leur taille et par la chaleur qu’ils dégagent.\nDes GPUs reconnus peuvent être les T5, A100, V100 et leur prix d’achat est de l’ordre de milliers d’euros, mais il faut bien prendre en compte également les coûts cachés. En effet, l’intégration dans un SI pré-existant peut nécessiter des travaux. Durant leur cycle de vie, ils ont besoin de maintenance. Et enfin, tout au long de leur utilisation, ils ont besoin d’être administrés, ce qui peut représenter des Equivalents Temps Plein (ETP), dont le coût n’est pas à négliger.\n\n\n\nIl est judicieux d’utiliser un orchestrateur pour déployer des Language Models (LLMs) dans une organisation pour plusieurs raisons :\n\nSimplification de la gestion des déploiements : un orchestrateur permet de gérer de manière centralisée tous les déploiements de LLMs dans l’organisation. Cela facilite la surveillance, la maintenance et la mise à l’échelle des déploiements.\nÉvolutivité : un orchestrateur permet de mettre à l’échelle automatiquement les déploiements en fonction de la demande, ce qui est particulièrement utile pour les LLMs qui peuvent être très gourmands en ressources.\nSécurité : un orchestrateur peut aider à renforcer la sécurité en fournissant des fonctionnalités telles que l’authentification, l’autorisation et le chiffrement des données. Il peut également aider à respecter les normes de conformité en matière de traitement des données.\nGestion des versions : un orchestrateur permet de gérer les versions des LLMs et de faciliter le déploiement de nouvelles versions ou de rollbacks en cas de problème.\nIntégration avec d’autres outils : un orchestrateur peut s’intégrer facilement avec d’autres outils de développement et d’exploitation, tels que les systèmes de surveillance, les outils de débogage et les systèmes de journalisation.\nRéduction des coûts : en automatisant les déploiements et en les mettant à l’échelle de manière efficace, un orchestrateur peut aider à réduire les coûts associés aux déploiements de LLMs.\n\nEn résumé, un orchestrateur offre une gestion centralisée, une évolutivité, une sécurité renforcée, une gestion des versions, une intégration avec d’autres outils et une réduction des coûts pour les déploiements de LLMs dans une organisation. Des solutions techniques peuvent être :\n\nKubernetes\nDocker Swarm\nApache Mesos\n\n\n\n\nNous allons développer dans cette partie un exemple de déploiement d’une structure LLM avec Kubernetes. On utilise la même structure de microservices que dans la partie précedente avec FastChat mais cela peut être adapté à tout choix d’organisation et d’architecture.\nVoici un schéma résumant l’organisation proposée ici, avec le controller, l’api openai-like et deux modèles LLMs :\n\n\n\nSchéma de structure des services pour Kubernetes\n\n\nLa méthodologie générale de l’utilisation de Kubernetes est la suivante :\n\nPréparer les images Docker qui seront utilisées pour les déploiements\nCréez les fichiers de configuration YAML pour votre application\nDéployez les avec :\n\nkubectl apply -f FILENAMES.yaml\n\nSurveiller le lancement des différents services et leur bonne interconnexion\n\nAvec cela, vous avez une application plus robuste, mais cela necessite une certaine familiarité avec Kubernetes. Quelques exemples de fichiers de configuration sont proposés ci-dessous.\n\nTout d’abord les services obligatoires comme le gestionnaire de l’API et le controlleur. On fait en même temps le deployment du pod et le service permettant d’y accéder. Ils se basent sur une image Docker légère et sans requirements spécifiques.\n\nOn remarquera que les deux deploiments semblent assez similaires et que la principale différence réside dans les noms donnés aux objets et à la commande lancée dans le conteneur lancé :\n[\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\nou\n[\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"dtnumds,dtnum-s56f1esfd,srp-sd5fze21d,dgfip-si2023\", \"--controller-address\", \"http://svc-controller\"]\nCes commandes sont celles de FastChat mais peuvent être remplacées par votre propre solution de déploiement de modèle.\nPour le controlleur :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-controller\n  template:\n    metadata:\n      labels:\n        app: fastchat-controller\n    spec:\n      containers:\n      - name: fastchat-controller\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 21001\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-llm-mixtral,svc-llm-e5-dgfip,svc-llm-llama\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-controller2\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30001\n      port: 80\n      targetPort: 21001\n  selector:\n    app: fastchat-controller\nEt pour l’api :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-openai\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-openai\n  template:\n    metadata:\n      labels:\n        app: fastchat-openai\n    spec:\n      containers:\n      - name: fastchat-openai\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller,svc-llm-mixtral\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"dtnumds,dtnum-s56f1esfd,srp-sd5fze21d,dgfip-si2023\", \"--controller-address\", \"http://svc-controller\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-openai-api\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30081\n      port: 80\n      targetPort: 8000\n  selector:\n    app: fastchat-openai\n\nCréez les fichiers de configuration pour un modèle LLM, avec également le pod et le service correspondant. Cette fois-ci, l’image est plus lourde car elle contient le modèle et les modules nécessaires à son fonctionnement.\n\nOn remarquera notamment :\nimage: fastchat-mixtral:v0.3.1\nresources:\n          limits:\n            nvidia.com/gpu: 2\nEt la commande qui lance le modèle (ici Fastchat mais pourrait être n’importe quel module):\ncommand: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-mixtral\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llm-mixtral\n  template:\n    metadata:\n      labels:\n        app: llm-mixtral\n    spec:\n      containers:\n      - name: llm-mixtral\n        image: fastchat-mixtral:v0.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 2100\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller\n        - name: CUDA_VISIBLE_DEVICES\n          value: \"0,1\"\n        command: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-llm-mixtral\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30091\n      port: 80\n      targetPort: 2100\n  selector:\n    app: llm-mixtral\nEnfin, tous ces composants se basent sur des images docker qui continennent tout le code de mise à disposition des modèles ou des APIs. Des exemples d’images utiles pour les différents services sus-mentionnés sont décrites dans ce Dockerfile :\n#################### BASE OPENAI IMAGE ####################\nFROM python:3.9-buster as llm-api-light\n\n# Set environment variables\nENV https_proxy=http://proxy.infra.dgfip:3128\nENV http_proxy=http://proxy.infra.dgfip:3128\nENV DEBIAN_FRONTEND noninteractive\n# Install dependencies\nRUN apt-get update -y && apt-get install -y curl\n# Install pip\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copy the FastChat directory into the Docker container\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n# Go into the FastChat directory and install from this directory\nWORKDIR /FastChat\nRUN pip3 install -e \".[webui]\" pydantic==1.10.13\nRUN pip3 install plotly\n\n#################### BASE LLM BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS base\nENV https_proxy=http://proxy.infra.dgfip:3128\nENV http_proxy=http://proxy.infra.dgfip:3128\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 curl\nRUN apt-get install -y python3-pip git\n# Copiez le répertoire FastChat dans le conteneur DockerEnfin, tous c\n\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install --upgrade pip\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.3.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-llama\nCOPY ./models/Upstage--Llama-2-70b-instruct-v2 /data/models/vllm/Upstage--Llama-2-70b-instruct-v2\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-mixtral\nCOPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1",
    "crumbs": [
      "III-Deploiements",
      "Socle Production"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Socle_Production.html#socle-pour-production-camille-jérôme-conrad",
    "href": "III-Deploiements/3_Socle_Production.html#socle-pour-production-camille-jérôme-conrad",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Les LLM sont des modèles de langage puissants qui nécessitent des ressources informatiques importantes pour fonctionner efficacement. Pour mettre en production une application utilisant des LLM, il est essentiel de choisir le bon matériel et les bons outils pour garantir la disponibilité des applications et des performances optimales.\nPour certains modèles de langage, les processeurs habituels appelés CPU (Central Processing Unit) peuvent suffire. Mais pour la plupart des modèles plus importants, pour que les calculs se fassent dans des temps raisonnables, il est nécessaire de se doter d’unités de traitement graphique (GPU). Nous allons donc nous intéresser ici aux différents critères à étudier pour choisir correctement des GPUs, aux outils qui permettent de suivre leurs performances et enfin le lien avec les essentiels de déploiements en termes de management de ressources (avec l’exemple du lien à Kubernetes).\n\n\nLa sélection des GPU (Graphics Processing Units) pour une installation dans une structure dépend de multiples facteurs. En voici quelques uns :\n\nPuissance de calcul : La puissance de traitement des GPU est mesurée en flops (floating-point operations per second). Un GPU plus puissant permettra d’exécuter des tâches plus rapidement et de gérer des charges de travail plus élevées.\nMémoire vive : La mémoire vive (VRAM) des GPU est essentielle pour les applications nécessitant une grande quantité de mémoire, comme les simulations scientifiques ou les applications de traitement d’image. Assurez-vous de choisir des GPU avec suffisamment de mémoire vive pour répondre aux besoins de vos applications.\nÉnergie et consommation : Les GPU consomment de l’énergie et génèrent de la chaleur. Choisissez des GPU économes en énergie et dotés de systèmes de refroidissement efficaces pour réduire les coûts énergétiques et améliorer la durée de vie des composants.\nCoût et rentabilité : Évaluez le coût total de possession (TCO) des GPU, en tenant compte des coûts d’achat, de maintenance et d’énergie. Choisissez des GPU qui offrent une bonne rentabilité pour votre administration.\nCompatibilité avec les systèmes d’exploitation : Vérifiez que les GPU sont compatibles avec les systèmes d’exploitation utilisés dans votre administration, tels que Windows, Linux ou macOS.\n\nPour le dernier point, il est commun d’acheter les GPUs par plusieurs, déjà groupés dans des serveurs. Il faut faire attention cependant au format et aux besoins spécifiques de ces serveurs, qui ne sont souvent pas standards par leur taille et par la chaleur qu’ils dégagent.\nDes GPUs reconnus peuvent être les T5, A100, V100 et leur prix d’achat est de l’ordre de milliers d’euros, mais il faut bien prendre en compte également les coûts cachés. En effet, l’intégration dans un SI pré-existant peut nécessiter des travaux. Durant leur cycle de vie, ils ont besoin de maintenance. Et enfin, tout au long de leur utilisation, ils ont besoin d’être administrés, ce qui peut représenter des Equivalents Temps Plein (ETP), dont le coût n’est pas à négliger.\n\n\n\nIl est judicieux d’utiliser un orchestrateur pour déployer des Language Models (LLMs) dans une organisation pour plusieurs raisons :\n\nSimplification de la gestion des déploiements : un orchestrateur permet de gérer de manière centralisée tous les déploiements de LLMs dans l’organisation. Cela facilite la surveillance, la maintenance et la mise à l’échelle des déploiements.\nÉvolutivité : un orchestrateur permet de mettre à l’échelle automatiquement les déploiements en fonction de la demande, ce qui est particulièrement utile pour les LLMs qui peuvent être très gourmands en ressources.\nSécurité : un orchestrateur peut aider à renforcer la sécurité en fournissant des fonctionnalités telles que l’authentification, l’autorisation et le chiffrement des données. Il peut également aider à respecter les normes de conformité en matière de traitement des données.\nGestion des versions : un orchestrateur permet de gérer les versions des LLMs et de faciliter le déploiement de nouvelles versions ou de rollbacks en cas de problème.\nIntégration avec d’autres outils : un orchestrateur peut s’intégrer facilement avec d’autres outils de développement et d’exploitation, tels que les systèmes de surveillance, les outils de débogage et les systèmes de journalisation.\nRéduction des coûts : en automatisant les déploiements et en les mettant à l’échelle de manière efficace, un orchestrateur peut aider à réduire les coûts associés aux déploiements de LLMs.\n\nEn résumé, un orchestrateur offre une gestion centralisée, une évolutivité, une sécurité renforcée, une gestion des versions, une intégration avec d’autres outils et une réduction des coûts pour les déploiements de LLMs dans une organisation. Des solutions techniques peuvent être :\n\nKubernetes\nDocker Swarm\nApache Mesos\n\n\n\n\nNous allons développer dans cette partie un exemple de déploiement d’une structure LLM avec Kubernetes. On utilise la même structure de microservices que dans la partie précedente avec FastChat mais cela peut être adapté à tout choix d’organisation et d’architecture.\nVoici un schéma résumant l’organisation proposée ici, avec le controller, l’api openai-like et deux modèles LLMs :\n\n\n\nSchéma de structure des services pour Kubernetes\n\n\nLa méthodologie générale de l’utilisation de Kubernetes est la suivante :\n\nPréparer les images Docker qui seront utilisées pour les déploiements\nCréez les fichiers de configuration YAML pour votre application\nDéployez les avec :\n\nkubectl apply -f FILENAMES.yaml\n\nSurveiller le lancement des différents services et leur bonne interconnexion\n\nAvec cela, vous avez une application plus robuste, mais cela necessite une certaine familiarité avec Kubernetes. Quelques exemples de fichiers de configuration sont proposés ci-dessous.\n\nTout d’abord les services obligatoires comme le gestionnaire de l’API et le controlleur. On fait en même temps le deployment du pod et le service permettant d’y accéder. Ils se basent sur une image Docker légère et sans requirements spécifiques.\n\nOn remarquera que les deux deploiments semblent assez similaires et que la principale différence réside dans les noms donnés aux objets et à la commande lancée dans le conteneur lancé :\n[\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\nou\n[\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"dtnumds,dtnum-s56f1esfd,srp-sd5fze21d,dgfip-si2023\", \"--controller-address\", \"http://svc-controller\"]\nCes commandes sont celles de FastChat mais peuvent être remplacées par votre propre solution de déploiement de modèle.\nPour le controlleur :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-controller\n  template:\n    metadata:\n      labels:\n        app: fastchat-controller\n    spec:\n      containers:\n      - name: fastchat-controller\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 21001\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-llm-mixtral,svc-llm-e5-dgfip,svc-llm-llama\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-controller2\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30001\n      port: 80\n      targetPort: 21001\n  selector:\n    app: fastchat-controller\nEt pour l’api :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-openai\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-openai\n  template:\n    metadata:\n      labels:\n        app: fastchat-openai\n    spec:\n      containers:\n      - name: fastchat-openai\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller,svc-llm-mixtral\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"dtnumds,dtnum-s56f1esfd,srp-sd5fze21d,dgfip-si2023\", \"--controller-address\", \"http://svc-controller\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-openai-api\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30081\n      port: 80\n      targetPort: 8000\n  selector:\n    app: fastchat-openai\n\nCréez les fichiers de configuration pour un modèle LLM, avec également le pod et le service correspondant. Cette fois-ci, l’image est plus lourde car elle contient le modèle et les modules nécessaires à son fonctionnement.\n\nOn remarquera notamment :\nimage: fastchat-mixtral:v0.3.1\nresources:\n          limits:\n            nvidia.com/gpu: 2\nEt la commande qui lance le modèle (ici Fastchat mais pourrait être n’importe quel module):\ncommand: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-mixtral\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llm-mixtral\n  template:\n    metadata:\n      labels:\n        app: llm-mixtral\n    spec:\n      containers:\n      - name: llm-mixtral\n        image: fastchat-mixtral:v0.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 2100\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller\n        - name: CUDA_VISIBLE_DEVICES\n          value: \"0,1\"\n        command: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-llm-mixtral\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30091\n      port: 80\n      targetPort: 2100\n  selector:\n    app: llm-mixtral\nEnfin, tous ces composants se basent sur des images docker qui continennent tout le code de mise à disposition des modèles ou des APIs. Des exemples d’images utiles pour les différents services sus-mentionnés sont décrites dans ce Dockerfile :\n#################### BASE OPENAI IMAGE ####################\nFROM python:3.9-buster as llm-api-light\n\n# Set environment variables\nENV https_proxy=http://proxy.infra.dgfip:3128\nENV http_proxy=http://proxy.infra.dgfip:3128\nENV DEBIAN_FRONTEND noninteractive\n# Install dependencies\nRUN apt-get update -y && apt-get install -y curl\n# Install pip\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copy the FastChat directory into the Docker container\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n# Go into the FastChat directory and install from this directory\nWORKDIR /FastChat\nRUN pip3 install -e \".[webui]\" pydantic==1.10.13\nRUN pip3 install plotly\n\n#################### BASE LLM BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS base\nENV https_proxy=http://proxy.infra.dgfip:3128\nENV http_proxy=http://proxy.infra.dgfip:3128\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 curl\nRUN apt-get install -y python3-pip git\n# Copiez le répertoire FastChat dans le conteneur DockerEnfin, tous c\n\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install --upgrade pip\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.3.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-llama\nCOPY ./models/Upstage--Llama-2-70b-instruct-v2 /data/models/vllm/Upstage--Llama-2-70b-instruct-v2\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-mixtral\nCOPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1",
    "crumbs": [
      "III-Deploiements",
      "Socle Production"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Socle_avance.html",
    "href": "III-Deploiements/2_Socle_avance.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Optimisation, Monitoring, UX/UI (CODE!)\n\n\nUne fois l’infrastructure sécurisée, il est toujours utile de monitorer les performances des GPU, pour suivre l’impact de cette technologie, pour monitorer la charge et prévenir de la surcharge. Idéallement, l’on peut aussi imaginer suivre la consommation projet par projet pour reporter les lignes de budget et faire des bilans carbonne.\nSelon les technologies de GPUs utilisées, il existe différents outils qui se conncectent aux infrastructure pour fournir des statistiques (notamment la mémoire utilisée, la bande passante et la température) :\n\nnvidia-smi\nAMD Vantage\nGPU-Z\n\nVoici un exemple de résultat de statistiques extraites d’une infrastructure GPUs :\n\n\n\nResultat de la commande nvidia-smi\n\n\nIl existe également d’autres moyens d’accéder à des GPUs que l’acquisition individuelle pour les administrations (voir Partie III.4).\n\n\n\nPlusieurs initiatives permettent de déployer rapidement des interfaces de chat avec des modèles LLMs, voire des applications de RAG avec back et front. On peut remarquer :\n\nla WebUI du module FastChat\nl’application CARADOC, mise en open source par l’équipe DataScience de la DTNUM de la DGFiP, publication prévue pour fin juin 2024.\n\nAperçu de l’application CARADOC pendant ses développements :\n\n\n\nInterface de l’application RAG Caradoc\n\n\nAperçu d’un interface possible avec FastChat :\n\n\n\nInterface de l’application Chat avec FastChat\n\n\nExemple de code pour lancer l’interface Gradio de FastChat dans un Docker :\nversion: \"3.9\"\nservices:\n  fastchat-gradio-server: \n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      FASTCHAT_CONTROLLER_URL: http://0.0.0.0:21001\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    image: fastchat:latest\n    ports:\n      - \"8001:8001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.gradio_web_server_dtnum\", \"--controller-url\", \"http://0.0.0.0:21001\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--model-list-mode\", \"reload\"]\nAvec toujours l’image Docker qui contient FastChat.",
    "crumbs": [
      "III-Deploiements",
      "Socle avancé"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Socle_avance.html#socle-avancé-camille-jérôme-conrad",
    "href": "III-Deploiements/2_Socle_avance.html#socle-avancé-camille-jérôme-conrad",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Optimisation, Monitoring, UX/UI (CODE!)\n\n\nUne fois l’infrastructure sécurisée, il est toujours utile de monitorer les performances des GPU, pour suivre l’impact de cette technologie, pour monitorer la charge et prévenir de la surcharge. Idéallement, l’on peut aussi imaginer suivre la consommation projet par projet pour reporter les lignes de budget et faire des bilans carbonne.\nSelon les technologies de GPUs utilisées, il existe différents outils qui se conncectent aux infrastructure pour fournir des statistiques (notamment la mémoire utilisée, la bande passante et la température) :\n\nnvidia-smi\nAMD Vantage\nGPU-Z\n\nVoici un exemple de résultat de statistiques extraites d’une infrastructure GPUs :\n\n\n\nResultat de la commande nvidia-smi\n\n\nIl existe également d’autres moyens d’accéder à des GPUs que l’acquisition individuelle pour les administrations (voir Partie III.4).\n\n\n\nPlusieurs initiatives permettent de déployer rapidement des interfaces de chat avec des modèles LLMs, voire des applications de RAG avec back et front. On peut remarquer :\n\nla WebUI du module FastChat\nl’application CARADOC, mise en open source par l’équipe DataScience de la DTNUM de la DGFiP, publication prévue pour fin juin 2024.\n\nAperçu de l’application CARADOC pendant ses développements :\n\n\n\nInterface de l’application RAG Caradoc\n\n\nAperçu d’un interface possible avec FastChat :\n\n\n\nInterface de l’application Chat avec FastChat\n\n\nExemple de code pour lancer l’interface Gradio de FastChat dans un Docker :\nversion: \"3.9\"\nservices:\n  fastchat-gradio-server: \n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      FASTCHAT_CONTROLLER_URL: http://0.0.0.0:21001\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    image: fastchat:latest\n    ports:\n      - \"8001:8001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.gradio_web_server_dtnum\", \"--controller-url\", \"http://0.0.0.0:21001\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--model-list-mode\", \"reload\"]\nAvec toujours l’image Docker qui contient FastChat.",
    "crumbs": [
      "III-Deploiements",
      "Socle avancé"
    ]
  },
  {
    "objectID": "I-Accompagnement/4_Impacts.html",
    "href": "I-Accompagnement/4_Impacts.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. Environnementaux (poids de l’entraînement, poids de l’utilisation)\nLe numérique est responsable de 2,5% de l’empreinte carbone de la France (17,2 Mt de CO2e & 20 millions de tonnes de déchets) selon l’étude ARCEP & ADEME de 2023. Par contre, il n’existe aucun référentiel à ce jour pour mesurer l’impact environnemental des projets d’intelligence artificielle. À titre d’exemple, les émissions liées à l’entraînement de GPT-3 sont estimées à 552 tonnes de CO2eq [1] et son utilisation en janvier 2023 représenterait 10 113 tonnes de CO2eq [2]. Les ressources en eau, métaux et d’autres matériaux pour la fabrication et opération des infrastructures sont également conséquents.\nAfin de permettre aux acteurs du numérique d’évaluer l’impact environnemental de leurs projets d’intelligence artificielle, et de communiquer sur le caractère frugal de ces derniers, l’Ecolab du MTECT prépare avec l’AFNOR un document de référence, qui devra être disponible en juillet.\nÀ l’heure actuelle, pour estimer la consommation énergétique et les émissions de CO2 liées à l’exécution du code, les data-scientists peuvent utiliser la librairie CodeCarbon, à mettre en place avant l’usage, et/ou Green Algorithms, qui peut être utilisé pour estimer un usage futur ou passé.\nLe coût environnementale lié aux infrastructures de calcul est mis à disposition par le groupe EcoInfo du CNRS à travers l’outil EcoDiag. Des estimations plus précises pour la fabrication de GPUs seront disponibles prochainement.\n[1] https://arxiv.org/pdf/2104.10350.pdf\n[2] Data For Good - Livre Blanc de l’IA Générative\nb. Légaux (RGPD, chartes de l’IA, IA Act, ...)\n\nc. Sécurité (Renvoyer vers le guide de l’ANSSI)",
    "crumbs": [
      "I-Accompagnement",
      "Impacts"
    ]
  },
  {
    "objectID": "I-Accompagnement/4_Impacts.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/4_Impacts.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. Environnementaux (poids de l’entraînement, poids de l’utilisation)\nLe numérique est responsable de 2,5% de l’empreinte carbone de la France (17,2 Mt de CO2e & 20 millions de tonnes de déchets) selon l’étude ARCEP & ADEME de 2023. Par contre, il n’existe aucun référentiel à ce jour pour mesurer l’impact environnemental des projets d’intelligence artificielle. À titre d’exemple, les émissions liées à l’entraînement de GPT-3 sont estimées à 552 tonnes de CO2eq [1] et son utilisation en janvier 2023 représenterait 10 113 tonnes de CO2eq [2]. Les ressources en eau, métaux et d’autres matériaux pour la fabrication et opération des infrastructures sont également conséquents.\nAfin de permettre aux acteurs du numérique d’évaluer l’impact environnemental de leurs projets d’intelligence artificielle, et de communiquer sur le caractère frugal de ces derniers, l’Ecolab du MTECT prépare avec l’AFNOR un document de référence, qui devra être disponible en juillet.\nÀ l’heure actuelle, pour estimer la consommation énergétique et les émissions de CO2 liées à l’exécution du code, les data-scientists peuvent utiliser la librairie CodeCarbon, à mettre en place avant l’usage, et/ou Green Algorithms, qui peut être utilisé pour estimer un usage futur ou passé.\nLe coût environnementale lié aux infrastructures de calcul est mis à disposition par le groupe EcoInfo du CNRS à travers l’outil EcoDiag. Des estimations plus précises pour la fabrication de GPUs seront disponibles prochainement.\n[1] https://arxiv.org/pdf/2104.10350.pdf\n[2] Data For Good - Livre Blanc de l’IA Générative\nb. Légaux (RGPD, chartes de l’IA, IA Act, ...)\n\nc. Sécurité (Renvoyer vers le guide de l’ANSSI)",
    "crumbs": [
      "I-Accompagnement",
      "Impacts"
    ]
  },
  {
    "objectID": "I-Accompagnement/3_Acculturation.html",
    "href": "I-Accompagnement/3_Acculturation.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Comment embarquer les métiers/personnels moins techniques\nPoints d'attention à partager sur l'utilisation de tels outils",
    "crumbs": [
      "I-Accompagnement",
      "Acculturation"
    ]
  },
  {
    "objectID": "I-Accompagnement/3_Acculturation.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/3_Acculturation.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "Comment embarquer les métiers/personnels moins techniques\nPoints d'attention à partager sur l'utilisation de tels outils",
    "crumbs": [
      "I-Accompagnement",
      "Acculturation"
    ]
  },
  {
    "objectID": "II-Developpements/3_Evaluations.html",
    "href": "II-Developpements/3_Evaluations.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Tous les LLM visent le même objectif : maîtriser le langage naturel et par là même, égaler l’humain dans des tâches telles que le résumé, la traduction, la reconnaissance des entités nommées, etc.\nCependant, tous les LLM souffrent des mêmes défauts, de façon plus ou moins prononcée:\n* Très grande sensibilité du modèle au prompt utilisé \n* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)\n* Les LLM peuvent avoir des comportements inattendus et dangereux suite à l'usage de prompts malveillants, de données \nd'entraînement biaisées, au recours à des agents trop permissifs, etc.\nOn souhaite donc se doter d’un cadre de comparaison qui permette d’affirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir à différentes métriques pour mesurer differents aspects du problème (fiabilité, sécurité, absence de biais…)\nSi de nombreux bancs d’essai existent aujourd’hui, permettant de distinguer certains LLM, il ne faut pas oublier que de bonnes performances dans un banc d’essai ne sont pas suffisantes, et qu’il est primordial de mettre en place un système d’évaluation quasi temps réél du LLM une fois en production.\n\n\n\na) Scenario\nUn scénario est un ensemble de conditions dans lesquelles la performance du LLM est évaluée. Il s’agit par exemple de\n\nRéponse aux questions\nRaisonnement\nTraduction\nGénération de texte\n…\n\nb) Tâche\nUne tâche constitue une forme plus granulaire d’un scénario. Elle conditionne plus spécifiquement sur quelle base le LLM est évalué. Une tâche peut être une composition de plusieurs sous-tâches.\n\nCombinaisons de sous-tâches de difficulté variée\n\nPar exemple, l’arithmétique peut être considérée comme une tâche constituée des sous-tâches arithmétique niveau 1er degré, arithmétique niveau collège, arithmétique niveau lycée, etc.\n\nCombinaison de sous-tâche de domaines variés\n\nLa tâche de type QCM peut être vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.\nc) Métrique\nUne métrique est une mesure qualitative utilisée pour évaluer la performance d’un modèle de langage dans certaines tâches/scénarios. Une métrique peut être :\n\nune fonction statistique/mathématique déterministe simple (par exemple, précision ou rappel)\nun score produit par un réseau neuronal ou un modèle de Machine Learning (ex. : score BERT)\nun score généré à l’aide d’un LLM (ex. : G-Eval)\n\nNotons que dans le dernier cas, évaluer un LLM à l’aide d’un LLM peut donner l’impression du serpent qui se mord la queue. Cependant, ce type de ‘dépendances circulaires’ existe couramment et est bien acceptée dans d’autres dommaines. Lors d’un entretien d’embauche par exemple, l’intellect humain évalue le potentiel d’un autre être humain.\nd) Benchmarks\nLes benchmarks sont des collections standardisées de tests utilisées pour évaluer les LLM sur une tâche ou un scénario donné. On trouvera pas exemple :\n\nSQuAD pour le scénario de réponse aux questions de l’utilisateur à partir d’extraction de parties d’un corpus (En anglais)\nPIAF semblable à SQuAD mais en français\nIMDB pour l’analyse des sentiments\n…\n\nA titre d’exemple, l’image ci-dessous présente le corpus PIAF. Il est composé de paragraphes issus d’articles de Wikipedia, et d’une liste de questions portant sur ces paragraphes.\n\n\n\n\nL’expression évaluation de LLM peut recouvrir différentes pratiques et différents objectifs. On doit ainsi distinguer l’évaluations de modèles LLM de l’évaluations de systèmes LLM. Les évaluations de modèles LLM s’intéressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacité sur un ensemble de tâches différentes.\nIl existe de nombreux benchmarks qui permettent d’illustrer les performances des modèles sur des aspects précis, comme HellaSwag (qui évalue la capacité d’un LLM à compléter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la véracité des réponses du modèle) et MMLU (qui mesure la capacité de compréhension et de résolution de problèmes ).\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nL’évaluation de systèmes LLM couvre l’évaluation de tous les composants de la chaîne, pour un modèle donné. En effet, un modèle de LLM est rarement utilisé seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le résultat du modèle. On pourra s’intéresser par exemple à l’effet du prompt sur la politesse de la réponse, le style, le niveau de détail, etc. Un modèle peut également recevoir un contexte (ensemble de documents, tableaux, images…) et son influence doit également être mesurée. On pourrait par exemple s’apercevoir que le modèle produit des résumés de qualité quand on lui fournit des documents littéraires, mais pas des documents techniques.\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nEn pratique, l’évaluation des modèles sur des benchmarks est réalisée par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communauté universitaire. Ce sont les évaluations de systèmes LLM qui intéresseront la majorité des équipes souhaitant déployer un LLM dans leur administration.\n\n\n\nIl n’existe pas de réponse simple à la question de savoir quelles métriques utiliser pour évaluer son système LLM. Cela dépendra du type de tâche, de la population cible, de la nature des données, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l’apprentissage machine, on évalue un modèle en se dotant d’un ensemble annoté d’entrées/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes réponses.\nLa difficulté de l’évaluation en IA générative réside dans le fait que nous ne disposons généralement pas de valeur de référence à laquelle comparer la sortie du modèle. Même dans les cas où l’on disposerait d’un exemple de bonne réponse, les données de sortie étant non structurées (texte en language naturel), il est difficile de comparer la distance entre deux objets.\nOn reprend ici l’idée de classer les métriques en fonction de leur approche du problème, c’est à dire comment elles évaluent la pertinence de la réponse obtenue. Certaines techniques supposent que l’on dispose d’une réponse de référence, et la question est alors de savoir comment elles évaluent la distance entre la réponse obtenue et une réponse de référence. D’autres techniques plus récentes ne font pas cette hypothèse, et cherchent à évaluer la qualité de la réponse dans l’absolu. \nOn ne détaillera pas toutes les métriques dans le cadre de ce guide, il existe pléthore de documentation disponible sur le sujet. L’objectif ici est plutôt de fournir une grille d’analyse.\nDans les métriques classiques, on trouve des métriques générales de classification qui sont couramment utilisées en apprentissage machine et ne sont pas propres aux données textuelles (Accuracy, Precision, Recall, F1…). Parmi les classiques, il existe également des métriques spécifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases prédites et les phrases de référence. Le recouvrement peut être calculé au niveau des mots, ou au niveau des lettres. Ces méthodes ont été critiquées par leur faible corrélation avec le jugement humain, ce qui est n’est pas étonnant dans la mesure où elles ne s’intéressent qu’a la forme de surface des mots.\nLes métriques basées sur le Deep Learning permettent de palier ce problème. Quand vient l’heure de choisir une métrique, on distingue en premier lieu les métriques qui ont besoin d’une valeur de référence et les autres. Détenir une valeur de référence peut représenter une grosse contrainte, et n’est pas toujours pertinent. Si on demande au LLM d’écrire un poème sur la mer par exemple, il serait vain de chercher à le comparer à un autre poème. Toujours est-il que si l’on dispose de valeurs de références, on peut recourir à des métriques basés sur les embeddings ou des métriques basées sur des modèles fine-tunés. Les métriques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexité peut les rendre intéressantes.\na. Métriques (biais, hallucinations, ...)\n\nb. Datasets\n\nc. Librairies/Frameworks (CODE!)\n\nd. Méthodologie (arbre de décision pour le décideur)",
    "crumbs": [
      "II-Developpements",
      "Evaluations"
    ]
  },
  {
    "objectID": "II-Developpements/3_Evaluations.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "href": "II-Developpements/3_Evaluations.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "title": "Guide du LLM",
    "section": "",
    "text": "Tous les LLM visent le même objectif : maîtriser le langage naturel et par là même, égaler l’humain dans des tâches telles que le résumé, la traduction, la reconnaissance des entités nommées, etc.\nCependant, tous les LLM souffrent des mêmes défauts, de façon plus ou moins prononcée:\n* Très grande sensibilité du modèle au prompt utilisé \n* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)\n* Les LLM peuvent avoir des comportements inattendus et dangereux suite à l'usage de prompts malveillants, de données \nd'entraînement biaisées, au recours à des agents trop permissifs, etc.\nOn souhaite donc se doter d’un cadre de comparaison qui permette d’affirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir à différentes métriques pour mesurer differents aspects du problème (fiabilité, sécurité, absence de biais…)\nSi de nombreux bancs d’essai existent aujourd’hui, permettant de distinguer certains LLM, il ne faut pas oublier que de bonnes performances dans un banc d’essai ne sont pas suffisantes, et qu’il est primordial de mettre en place un système d’évaluation quasi temps réél du LLM une fois en production.\n\n\n\na) Scenario\nUn scénario est un ensemble de conditions dans lesquelles la performance du LLM est évaluée. Il s’agit par exemple de\n\nRéponse aux questions\nRaisonnement\nTraduction\nGénération de texte\n…\n\nb) Tâche\nUne tâche constitue une forme plus granulaire d’un scénario. Elle conditionne plus spécifiquement sur quelle base le LLM est évalué. Une tâche peut être une composition de plusieurs sous-tâches.\n\nCombinaisons de sous-tâches de difficulté variée\n\nPar exemple, l’arithmétique peut être considérée comme une tâche constituée des sous-tâches arithmétique niveau 1er degré, arithmétique niveau collège, arithmétique niveau lycée, etc.\n\nCombinaison de sous-tâche de domaines variés\n\nLa tâche de type QCM peut être vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.\nc) Métrique\nUne métrique est une mesure qualitative utilisée pour évaluer la performance d’un modèle de langage dans certaines tâches/scénarios. Une métrique peut être :\n\nune fonction statistique/mathématique déterministe simple (par exemple, précision ou rappel)\nun score produit par un réseau neuronal ou un modèle de Machine Learning (ex. : score BERT)\nun score généré à l’aide d’un LLM (ex. : G-Eval)\n\nNotons que dans le dernier cas, évaluer un LLM à l’aide d’un LLM peut donner l’impression du serpent qui se mord la queue. Cependant, ce type de ‘dépendances circulaires’ existe couramment et est bien acceptée dans d’autres dommaines. Lors d’un entretien d’embauche par exemple, l’intellect humain évalue le potentiel d’un autre être humain.\nd) Benchmarks\nLes benchmarks sont des collections standardisées de tests utilisées pour évaluer les LLM sur une tâche ou un scénario donné. On trouvera pas exemple :\n\nSQuAD pour le scénario de réponse aux questions de l’utilisateur à partir d’extraction de parties d’un corpus (En anglais)\nPIAF semblable à SQuAD mais en français\nIMDB pour l’analyse des sentiments\n…\n\nA titre d’exemple, l’image ci-dessous présente le corpus PIAF. Il est composé de paragraphes issus d’articles de Wikipedia, et d’une liste de questions portant sur ces paragraphes.\n\n\n\n\nL’expression évaluation de LLM peut recouvrir différentes pratiques et différents objectifs. On doit ainsi distinguer l’évaluations de modèles LLM de l’évaluations de systèmes LLM. Les évaluations de modèles LLM s’intéressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacité sur un ensemble de tâches différentes.\nIl existe de nombreux benchmarks qui permettent d’illustrer les performances des modèles sur des aspects précis, comme HellaSwag (qui évalue la capacité d’un LLM à compléter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la véracité des réponses du modèle) et MMLU (qui mesure la capacité de compréhension et de résolution de problèmes ).\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nL’évaluation de systèmes LLM couvre l’évaluation de tous les composants de la chaîne, pour un modèle donné. En effet, un modèle de LLM est rarement utilisé seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le résultat du modèle. On pourra s’intéresser par exemple à l’effet du prompt sur la politesse de la réponse, le style, le niveau de détail, etc. Un modèle peut également recevoir un contexte (ensemble de documents, tableaux, images…) et son influence doit également être mesurée. On pourrait par exemple s’apercevoir que le modèle produit des résumés de qualité quand on lui fournit des documents littéraires, mais pas des documents techniques.\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nEn pratique, l’évaluation des modèles sur des benchmarks est réalisée par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communauté universitaire. Ce sont les évaluations de systèmes LLM qui intéresseront la majorité des équipes souhaitant déployer un LLM dans leur administration.\n\n\n\nIl n’existe pas de réponse simple à la question de savoir quelles métriques utiliser pour évaluer son système LLM. Cela dépendra du type de tâche, de la population cible, de la nature des données, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l’apprentissage machine, on évalue un modèle en se dotant d’un ensemble annoté d’entrées/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes réponses.\nLa difficulté de l’évaluation en IA générative réside dans le fait que nous ne disposons généralement pas de valeur de référence à laquelle comparer la sortie du modèle. Même dans les cas où l’on disposerait d’un exemple de bonne réponse, les données de sortie étant non structurées (texte en language naturel), il est difficile de comparer la distance entre deux objets.\nOn reprend ici l’idée de classer les métriques en fonction de leur approche du problème, c’est à dire comment elles évaluent la pertinence de la réponse obtenue. Certaines techniques supposent que l’on dispose d’une réponse de référence, et la question est alors de savoir comment elles évaluent la distance entre la réponse obtenue et une réponse de référence. D’autres techniques plus récentes ne font pas cette hypothèse, et cherchent à évaluer la qualité de la réponse dans l’absolu. \nOn ne détaillera pas toutes les métriques dans le cadre de ce guide, il existe pléthore de documentation disponible sur le sujet. L’objectif ici est plutôt de fournir une grille d’analyse.\nDans les métriques classiques, on trouve des métriques générales de classification qui sont couramment utilisées en apprentissage machine et ne sont pas propres aux données textuelles (Accuracy, Precision, Recall, F1…). Parmi les classiques, il existe également des métriques spécifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases prédites et les phrases de référence. Le recouvrement peut être calculé au niveau des mots, ou au niveau des lettres. Ces méthodes ont été critiquées par leur faible corrélation avec le jugement humain, ce qui est n’est pas étonnant dans la mesure où elles ne s’intéressent qu’a la forme de surface des mots.\nLes métriques basées sur le Deep Learning permettent de palier ce problème. Quand vient l’heure de choisir une métrique, on distingue en premier lieu les métriques qui ont besoin d’une valeur de référence et les autres. Détenir une valeur de référence peut représenter une grosse contrainte, et n’est pas toujours pertinent. Si on demande au LLM d’écrire un poème sur la mer par exemple, il serait vain de chercher à le comparer à un autre poème. Toujours est-il que si l’on dispose de valeurs de références, on peut recourir à des métriques basés sur les embeddings ou des métriques basées sur des modèles fine-tunés. Les métriques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexité peut les rendre intéressantes.\na. Métriques (biais, hallucinations, ...)\n\nb. Datasets\n\nc. Librairies/Frameworks (CODE!)\n\nd. Méthodologie (arbre de décision pour le décideur)",
    "crumbs": [
      "II-Developpements",
      "Evaluations"
    ]
  },
  {
    "objectID": "II-Developpements/1_Revue_Technique_LLM.html",
    "href": "II-Developpements/1_Revue_Technique_LLM.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. Architectures principales LLM\n\nb. Méthodes de fine-tuning  (Conrad)\n\nLes LLM sont des réseaux de neurones de taille importante et font l'objet d'entraînement avec des ressources colossales (*e.g*: quelques dizaines de milliers de GPUs dernier modèle pendant 3 mois pour `GPT-4`). L'entraînement permet d'apprendre un jeu de données particulier, en réglant l'ensemble des poids du modèles (*e.g*: `Mixtral 8x22B` est une architecture à 141 milliards de poids; 175 milliards pour `GPT-3`). Les LLM sont entraînés à répondre à plusieurs tâches génériques et ne sont pas forcément pertinent pour des cas d'utilisation particulier.\n\nPour répondre à ce besoin, plusieurs méthodes relevant du principe de fine-tuning sont possibles. Le fine-tuning consiste à reprendre un modèle déjà entraîné et à l'adapter sur un jeu de données particulier sur une ou plusieurs tâches spécifiques. En général, il s'agit de modifier une partie ou l'ensemble des poids pour que le modèle soit plus précis pour les tâches voulues. Le fine-tuning garde en grande partie les bénéfices de l'entraînement initial, *i.e* les connaissances antérieurs déjà apprises. Repartir d'un modèle déjà entraîné pourra réduire le temps d'entraînement requis pour le fine-tuning, en fonction de la similarité entre la nouvelle tâche souhaitée et son jeu de données et les entraînements précédents.\n\nPour des petits modèles de langages, il est possible de ré-entraîner en modifiant l'ensemble des poids. Pour des modèles plus grands, modifier l'ensemble des poids peut s'avérer couteux en temps et en GPUs. Plusieurs approches permettent de ré-entraîner à moindre coût :\n* réentrainer seulement un sous-ensemble de poids\n* modifier la tête de modélisation de la langue (`lm_head`) pour certains modèles, soit en réentrainant depuis les poids entraînés, soit en réinitialisant ces poids.\n* garder l'intégralité du modèle et rajouter des poids à entraîner puis utiliser l'approximation de bas rang avec `LORA` (`Low-Rank Adaptation`) pour l'entraînement et l'inférence.\n* utiliser des versions quantisées, i.e. des modèles où les poids ont été tronqués à une précision inférieure (possibilité de combiner avec la technique précédente, sous le nom de qLORA).\n\nEntraînement avec qLORA en pratique :\n\nEn plus de la librairie `transformers` et `datasets`, les librairies `peft`, `bitsandbytes` et `trl` permettent de simplifier l'entraînement avec qLORA\n\n(inspiré du [notebook suivant](https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning) )\n\n```python\n%%capture %pip install -U bitsandbytes %pip install -U transformers %pip install -U peft %pip install -U trl %pip install -U sentencepiece %pip install -U protobuf\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model from datasets import load_dataset import torch from trl import SFTTrainer\nbase_model = “teknium/OpenHermes-2.5-Mistral-7B” new_model = “Mistral-7b-instruct-teletravail”\npath_to_training_file=“Dataset_public_accords_teletravail_Dares_train.parquet” path_to_test_file=“Dataset_public_accords_teletravail_Dares_test.parquet”\ndataset=load_dataset(“parquet”, data_files={‘train’: path_to_training_file, ‘test’: path_to_test_file})\nbnb_config = BitsAndBytesConfig( load_in_4bit= True, bnb_4bit_quant_type= “nf4”, bnb_4bit_compute_dtype= torch.bfloat16, bnb_4bit_use_double_quant= False, )\nmodel = AutoModelForCausalLM.from_pretrained( base_model, quantization_config=bnb_config, torch_dtype=torch.bfloat16, device_map=“auto”, trust_remote_code=True, ) model.config.use_cache = False # silence the warnings. Please re-enable for inference! model.config.pretraining_tp = 1 model.gradient_checkpointing_enable()",
    "crumbs": [
      "II-Developpements",
      "Revue technique des LLM"
    ]
  },
  {
    "objectID": "II-Developpements/1_Revue_Technique_LLM.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "href": "II-Developpements/1_Revue_Technique_LLM.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. Architectures principales LLM\n\nb. Méthodes de fine-tuning  (Conrad)\n\nLes LLM sont des réseaux de neurones de taille importante et font l'objet d'entraînement avec des ressources colossales (*e.g*: quelques dizaines de milliers de GPUs dernier modèle pendant 3 mois pour `GPT-4`). L'entraînement permet d'apprendre un jeu de données particulier, en réglant l'ensemble des poids du modèles (*e.g*: `Mixtral 8x22B` est une architecture à 141 milliards de poids; 175 milliards pour `GPT-3`). Les LLM sont entraînés à répondre à plusieurs tâches génériques et ne sont pas forcément pertinent pour des cas d'utilisation particulier.\n\nPour répondre à ce besoin, plusieurs méthodes relevant du principe de fine-tuning sont possibles. Le fine-tuning consiste à reprendre un modèle déjà entraîné et à l'adapter sur un jeu de données particulier sur une ou plusieurs tâches spécifiques. En général, il s'agit de modifier une partie ou l'ensemble des poids pour que le modèle soit plus précis pour les tâches voulues. Le fine-tuning garde en grande partie les bénéfices de l'entraînement initial, *i.e* les connaissances antérieurs déjà apprises. Repartir d'un modèle déjà entraîné pourra réduire le temps d'entraînement requis pour le fine-tuning, en fonction de la similarité entre la nouvelle tâche souhaitée et son jeu de données et les entraînements précédents.\n\nPour des petits modèles de langages, il est possible de ré-entraîner en modifiant l'ensemble des poids. Pour des modèles plus grands, modifier l'ensemble des poids peut s'avérer couteux en temps et en GPUs. Plusieurs approches permettent de ré-entraîner à moindre coût :\n* réentrainer seulement un sous-ensemble de poids\n* modifier la tête de modélisation de la langue (`lm_head`) pour certains modèles, soit en réentrainant depuis les poids entraînés, soit en réinitialisant ces poids.\n* garder l'intégralité du modèle et rajouter des poids à entraîner puis utiliser l'approximation de bas rang avec `LORA` (`Low-Rank Adaptation`) pour l'entraînement et l'inférence.\n* utiliser des versions quantisées, i.e. des modèles où les poids ont été tronqués à une précision inférieure (possibilité de combiner avec la technique précédente, sous le nom de qLORA).\n\nEntraînement avec qLORA en pratique :\n\nEn plus de la librairie `transformers` et `datasets`, les librairies `peft`, `bitsandbytes` et `trl` permettent de simplifier l'entraînement avec qLORA\n\n(inspiré du [notebook suivant](https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning) )\n\n```python\n%%capture %pip install -U bitsandbytes %pip install -U transformers %pip install -U peft %pip install -U trl %pip install -U sentencepiece %pip install -U protobuf\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model from datasets import load_dataset import torch from trl import SFTTrainer\nbase_model = “teknium/OpenHermes-2.5-Mistral-7B” new_model = “Mistral-7b-instruct-teletravail”\npath_to_training_file=“Dataset_public_accords_teletravail_Dares_train.parquet” path_to_test_file=“Dataset_public_accords_teletravail_Dares_test.parquet”\ndataset=load_dataset(“parquet”, data_files={‘train’: path_to_training_file, ‘test’: path_to_test_file})\nbnb_config = BitsAndBytesConfig( load_in_4bit= True, bnb_4bit_quant_type= “nf4”, bnb_4bit_compute_dtype= torch.bfloat16, bnb_4bit_use_double_quant= False, )\nmodel = AutoModelForCausalLM.from_pretrained( base_model, quantization_config=bnb_config, torch_dtype=torch.bfloat16, device_map=“auto”, trust_remote_code=True, ) model.config.use_cache = False # silence the warnings. Please re-enable for inference! model.config.pretraining_tp = 1 model.gradient_checkpointing_enable()",
    "crumbs": [
      "II-Developpements",
      "Revue technique des LLM"
    ]
  },
  {
    "objectID": "Guide.html",
    "href": "Guide.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Vision high level de l'intérêt des LLMs\n\n\n\ncas d’usage\ndes infras\ndes modeles\ndes datasets/open data ?\n\n\n\nComment embarquer les métiers/personnels moins techniques\nPoints d'attention à partager sur l'utilisation de tels outils\n\n\n\na. Environnementaux (poids de l’entraînement, poids de l’utilisation)\n\nb. Légaux (RGPD, chartes de l’IA, IA Act, ...)\n\nc. Sécurité (Renvoyer vers le guide de l’ANSSI)\n\n\n\n\n\n\na. Architectures principales LLM\n\nb. Méthodes de fine-tuning  (Conrad)\n\nc. Prompt engineer (lien vers prompt guide)\n\nd. Quoi faire quand ? (arbre de décision)\n\n\n\na. pipelines,\n\nb. Benchmark des différentes bases vectorielles (Katia)\n\nc. presentations de modules avec RETEX, CODE!)\n\n\n\na. Métriques (biais, hallucinations, ...)\n\nb. Datasets\n\nc. Librairies/Frameworks (CODE!)\n\nd. Méthodologie (arbre de décision pour le décideur)\n\n\n\n\n\n\na. vLLM + docker ? (CODE!)\n\n\n\nOptimisation, Monitoring, UX/UI (CODE!)\n\n\n\nOrchestrateur (CODE!)\n\nNvidia etc.\n\n\n\nSSP Cloud (Onyxia)\n\nCloud PI\n\nClouds privés (SECNUMCLOUD, Sens?)\n\n(NuboNyxia à terme)"
  },
  {
    "objectID": "Guide.html#partie-i.-accompagnement-au-changement",
    "href": "Guide.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "Vision high level de l'intérêt des LLMs\n\n\n\ncas d’usage\ndes infras\ndes modeles\ndes datasets/open data ?\n\n\n\nComment embarquer les métiers/personnels moins techniques\nPoints d'attention à partager sur l'utilisation de tels outils\n\n\n\na. Environnementaux (poids de l’entraînement, poids de l’utilisation)\n\nb. Légaux (RGPD, chartes de l’IA, IA Act, ...)\n\nc. Sécurité (Renvoyer vers le guide de l’ANSSI)"
  },
  {
    "objectID": "Guide.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "href": "Guide.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. Architectures principales LLM\n\nb. Méthodes de fine-tuning  (Conrad)\n\nc. Prompt engineer (lien vers prompt guide)\n\nd. Quoi faire quand ? (arbre de décision)\n\n\n\na. pipelines,\n\nb. Benchmark des différentes bases vectorielles (Katia)\n\nc. presentations de modules avec RETEX, CODE!)\n\n\n\na. Métriques (biais, hallucinations, ...)\n\nb. Datasets\n\nc. Librairies/Frameworks (CODE!)\n\nd. Méthodologie (arbre de décision pour le décideur)\n\n\n\n\n\n\na. vLLM + docker ? (CODE!)\n\n\n\nOptimisation, Monitoring, UX/UI (CODE!)\n\n\n\nOrchestrateur (CODE!)\n\nNvidia etc.\n\n\n\nSSP Cloud (Onyxia)\n\nCloud PI\n\nClouds privés (SECNUMCLOUD, Sens?)\n\n(NuboNyxia à terme)"
  },
  {
    "objectID": "II-Developpements/2_RAG.html",
    "href": "II-Developpements/2_RAG.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. pipelines,\n\nb. presentations de modules avec RETEX, CODE!)",
    "crumbs": [
      "II-Developpements",
      "Retrieval Augmented Generation"
    ]
  },
  {
    "objectID": "II-Developpements/2_RAG.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "href": "II-Developpements/2_RAG.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. pipelines,\n\nb. presentations de modules avec RETEX, CODE!)",
    "crumbs": [
      "II-Developpements",
      "Retrieval Augmented Generation"
    ]
  },
  {
    "objectID": "I-Accompagnement/1_Besoins.html",
    "href": "I-Accompagnement/1_Besoins.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Vision high level de l'intérêt des LLMs\nLes cas d’usages des LLMs sont variés et avant de se lancer et innover grâce aux LLMs, il est nécessaire de bien identifier le besoin qui amène l’utilisation d’un LLM. Pour quoi faire ? Pour quels usages ? Est-ce pour de la génération de texte ? Pour de la classification ? L’objectif de ce chapitre est d’accompagner la réflexion autour de l’identification du besoin et de la collecte des données, avec les différents types de cas d’usages impliquant des LLMs.\nLes cas d’usages :\n\ncas d’usages autour de la génération de contenu\ncas d’usage autour de la classification et de la recherche de contenu\ncas d’usage autour des interactions conversationnelles",
    "crumbs": [
      "I-Accompagnement",
      "Besoins"
    ]
  },
  {
    "objectID": "I-Accompagnement/1_Besoins.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/1_Besoins.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "Vision high level de l'intérêt des LLMs\nLes cas d’usages des LLMs sont variés et avant de se lancer et innover grâce aux LLMs, il est nécessaire de bien identifier le besoin qui amène l’utilisation d’un LLM. Pour quoi faire ? Pour quels usages ? Est-ce pour de la génération de texte ? Pour de la classification ? L’objectif de ce chapitre est d’accompagner la réflexion autour de l’identification du besoin et de la collecte des données, avec les différents types de cas d’usages impliquant des LLMs.\nLes cas d’usages :\n\ncas d’usages autour de la génération de contenu\ncas d’usage autour de la classification et de la recherche de contenu\ncas d’usage autour des interactions conversationnelles",
    "crumbs": [
      "I-Accompagnement",
      "Besoins"
    ]
  },
  {
    "objectID": "I-Accompagnement/2_Deja_Fait_Admin.html",
    "href": "I-Accompagnement/2_Deja_Fait_Admin.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "cas d’usage\n\nDans une enquête incluant un champ « Commentaire », celui-ci peut être analysé par des LLMs afin d’identifier les thématiques saillantes exprimées dans ce champ. Ensuite, pour chacune de ces thématiques, les LLMs peuvent être utilisés pour dégager le sentiment prédominant (ex : positif, négatif, neutre) associé à chacune d’entre elles. In fine, grâce aux LLMs, le champ « Commentaire » peut ainsi être divisé en un nombre N de thématiques, et, pour chacune de ces thématiques, un contenu peut être généré afin de faire ressortir le sentiment majoritaire des répondants à l’enquête.\ndes infras des modeles des datasets/open data ?",
    "crumbs": [
      "I-Accompagnement",
      "Exemples dans l'administration"
    ]
  },
  {
    "objectID": "I-Accompagnement/2_Deja_Fait_Admin.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/2_Deja_Fait_Admin.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "cas d’usage\n\nDans une enquête incluant un champ « Commentaire », celui-ci peut être analysé par des LLMs afin d’identifier les thématiques saillantes exprimées dans ce champ. Ensuite, pour chacune de ces thématiques, les LLMs peuvent être utilisés pour dégager le sentiment prédominant (ex : positif, négatif, neutre) associé à chacune d’entre elles. In fine, grâce aux LLMs, le champ « Commentaire » peut ainsi être divisé en un nombre N de thématiques, et, pour chacune de ces thématiques, un contenu peut être généré afin de faire ressortir le sentiment majoritaire des répondants à l’enquête.\ndes infras des modeles des datasets/open data ?",
    "crumbs": [
      "I-Accompagnement",
      "Exemples dans l'administration"
    ]
  },
  {
    "objectID": "III-Deploiements/4_Infras_administrations.html",
    "href": "III-Deploiements/4_Infras_administrations.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Guide du LLM\n\nPARTIE III. Deploiements\n\n\n4. Infras dispos pour l’administration (Thibault Katia)\nSSP Cloud (Onyxia)\n\nCloud PI\n\nClouds privés (SECNUMCLOUD, Sens?)\n\n(NuboNyxia à terme)",
    "crumbs": [
      "III-Deploiements",
      "Infrastructures dans l'administration"
    ]
  },
  {
    "objectID": "III-Deploiements/1_Socle_minimal.html",
    "href": "III-Deploiements/1_Socle_minimal.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Pour déployer un grand modèle de langage (LLM) dans une infrastructure, il est essentiel de comprendre comment requêter le modèle, les quelques couches techniques immédiates qui l’entourent et les solutions disponibles pour un déploiement efficace.\n\n\nLorsqu’il s’agit de mettre en service des applications basées sur des LLM, il y a 2 composants principaux : le moteur et le serveur. Le moteur gère tout ce qui concerne les modèles et le regroupement des demandes, tandis que le serveur gère l’acheminement des demandes des utilisateurs.\n\n\nLes moteurs sont les composants exécutant les modèles et tout ce que nous avons couvert jusqu’à présent sur le processus de génération avec différents types d’optimisations. À leur cœur, ce sont des bibliothèques Python. Ils gèrent le regroupement des demandes qui proviennent des utilisateurs vers notre chatbot et génèrent la réponse à ces demandes.\n\n\n\nLes serveurs sont responsables de l’orchestration des requêtes HTTP/gRPC entrantes des utilisateurs. Dans les applications du monde réel, nous aurons de nombreux utilisateurs qui posent des questions à notre chatbot à différents moments de la journée. Les serveurs mettent ces demandes en file d’attente et les transfèrent vers le moteur pour la génération de la réponse. Les serveurs apportent également les métriques telles que le débit et la latence, qui sont importantes à suivre pour le service de modèle.\n\n\n\nMoteurs\n\nOptimisation de la mémoire\nOptimisation spécifique au modèle\nPrise en charge du regroupement\n\nServeurs\n\nAPI HTTP/gRPC\nMise en file d’attente des demandes\nMise en service de plusieurs modèles\nPrise en charge de plusieurs moteurs\n\n\n\n\n\nQuels outils sont les mieux adaptés à nos besoins ? Comment choisir ? Voici un survol rapide de grands noms du milieu pour références.\n\nUne recommandation de framework rapide à prendre en main et dont l’utilité a déjà été prouvée dans une de nos administrations se trouve à la fin et est développée dans le prochain paragraphe.\n\nMoteurs\n\nTensorRT-LLM est une bibliothèque open-source qui optimise les performances d’inférence des grands modèles de langage (LLM) en utilisant les GPU NVIDIA Tensor Core. Elle utilise le parallélisme tensoriel, propose une API Python simple et comprend des versions optimisées de LLM populaires. Elle prend en charge le batching en vol et vise à simplifier la construction et l’expérimentation de nouveaux LLM. Cependant, les utilisateurs doivent spécifier la longueur d’entrée/sortie maximale et la taille de lot avant de construire le moteur, et la gestion de la mémoire du cache KV n’est pas open source.\nvLLM est une bibliothèque à hautes performances pour l’inférence et le service LLM, axée sur le débit de service et l’efficacité mémoire grâce à son mécanisme PagedAttention. Il prend en charge le batching continu, le parallélisme GPU et la sortie en streaming, ainsi que la compatibilité OpenAI. Cependant, la mémoire peut devenir un goulot d’étranglement avec des taux de demande élevés et de grandes tailles de lot.\n\nServeurs\n\nRayLLM avec RayServe est construit sur un framework de calcul distribué qui simplifie le développement et le déploiement de modèles d’IA à grande échelle. Il prend en charge les points de terminaison multi-modèles, les fonctionnalités serveur et les optimisations via les intégrations avec vLLM et TGI.\nTriton avec TensorRT-LLM fournit un logiciel d’inférence de serveur pour le déploiement et l’exécution efficaces de LLM avec des techniques telles que le batching en vol et le cache KV paginé.\n\nMoteurs et serveurs\n\nGénération de texte Inférence (TGI) est un serveur Rust, Python et gRPC utilisé chez HuggingFace pour HuggingChat, l’API d’inférence et le point de terminaison d’inférence. Il prend en charge le batching continu, le parallélisme tensoriel, la quantification, les mécanismes d’attention, le recuit simulé des logits et des LLM spécifiques. Cependant, la licence d’utilisation a été modifiée et n’est pas gratuite pour une utilisation commerciale.\nEnfin, Fastchat est une solution auto-hébergée pour héberger des modèles d’IA génératifs et qui propose la gestion des modèles, des API OpenAI-compatibles et une web interface simple.\n\n\nNous allons développer FastChat dans la partie suivante car c’est un outil qui a été testé et qui semble fournir beaucoup des éléments nécessaires pour une utilisation de première intention.\n\n\n\n\n\n\nPour certains cas d’usage, l’enjeu est de traiter de nombreuses données avec le même mode opératoire en un coup de manière ponctuelle. C’est ce qu’on appellera le traitement par batch. Cela consiste à charger un modèle, le requêter sur un tableau de prompt et obtenir la sortie pour pouvoir l’exporter. On peut le faire avec vLLM par exemple avec un morceau de code de ce type :\nfrom vllm import LLM, SamplingParams\nimport re\nimport pandas as pd\nimport re\nimport json\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n\nlist_data = json.load(open(\"Data.json\"))\n\nlist_prompts = [ v for x,v in list_data.items()]\nlist_ids = [ x for x,v in list_data.items() ]\n\nsampling_params = SamplingParams(temperature=0.1, top_p=0.1, max_tokens=4096)\nllm = LLM(model=\"/data/models/hub/models--upstage--Llama-2-70b-instruct-v2/snapshots/36b2a974642846b40fbbafaabad936cd6f8a7632\", tensor_parallel_size=2)\nprint(\"STARTING INFERENCE\")\noutputs = llm.generate(list_prompts, sampling_params)\n\nresume = { idx:output.outputs[0].text for idx, output in zip(list_ids, outputs) }\n\njson.dump(resume, open(\"Sortie.json\", \"w\"))\nMais cette méthodologie a des limites, car cela nécessite de bloquer des gpus, ce qui entraîne des problématiques de gestion et de partage.\n\n\n\nQue ce soit une équipe de plusieurs data-scientists, ou un ensemble d’application, si les besoins sont importants, les GPUs ont tout intérêt à être partagés. Il ne sera donc pas possible que chaque script python charge son modèle en mémoire et bloque des GPUs. Il est également plus rassurant de séparer l’infrastructure GPU des utilisateurs pour que chacun travaille dans son environnement, afin d’éviter les casses accidentelles.\nLa solution qui consiste à mettre à disposition des APIs vient répondre à ces problématiques. Les modèles sont cachés derrière les API, les datascientist et les applications peuvent venir les requêter et n’ont pas besoin de s’occuper de l’infrastructure. Ainsi, plutôt que chaque datascientist déploie un même modèle avec réservation de GPU, l’architecture en API permet la mise en commun du déploiement au même besoin.\n\nDans ce guide, FastChat est présenté comme exemple pour la simplicité mais d’autres solutions existent, avec chacunes leurs avantages et inconvénients.\n\n\n\n\n\nFastChat propose des API OpenAI-compatibles pour ses modèles pris en charge, de sorte que vous puissiez utiliser FastChat comme une alternative locale aux API OpenAI. Cela permet d’utiliser la bibliothèque openai-python et les commandes cURL, ce qui facilite le travail des datascientists.\nLa documentation complète est disponible sur le repo du module : https://github.com/lm-sys/FastChat/tree/main\nNous allons tout de même parcourir les grandes étapes pour pouvoir lancer son installation et ensuite l’utiliser.\n\n\nTout repose sur la complémentarité de trois services : le controller, les modèles et l’API. Il faut commencer par lancer le controller.\npython3 -m fastchat.serve.controller\nEnsuite, les model_workers. (Un modèle vicuna est pris pour l’exemple.)\npython3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5\nEt enfin, l’API.\npython3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n\n\n\nLe but de openai_api_server.py est d’implémenter un serveur d’API entièrement compatible avec OpenAI, de sorte que les modèles puissent être utilisés directement avec la bibliothèque openai-python.\nTout d’abord, installez le package Python OpenAI &gt;= 1.0 :\npip install --upgrade openai\nEnsuite, interagissez avec le modèle Vicuna :\nimport openai\n\nopenai.api_key = \"EMPTY\"\nopenai.base_url = \"http://localhost:8000/v1/\"\n\nmodel = \"vicuna-7b-v1.5\"\nprompt = \"Il était une fois\"\n\n# créer une complétion\ncompletion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)\n# imprimer la complétion\nprint(prompt + completion.choices[0].text)\n\n# créer une complétion de chat\ncompletion = openai.chat.completions.create(\n  model=model,\n  messages=[{\"role\": \"user\", \"content\": \"Bonjour ! Quel est votre nom ?\"}]\n)\n# imprimer la complétion\nprint(completion.choices[0].message.content)\n\n\n\ncURL est un autre bon outil pour observer la sortie de l’API.\nList Models:\ncurl http://localhost:8000/v1/models\nChat Completions:\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n  }'\nText Completions:\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"prompt\": \"Once upon a time\",\n    \"max_tokens\": 41,\n    \"temperature\": 0.5\n  }'\nEmbeddings:\ncurl http://localhost:8000/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"input\": \"Hello world!\"\n  }'\n\n\n\nVous pouvez utiliser vLLM comme une mise en œuvre optimisée d’un travailleur dans FastChat. Il offre une mise en batch continue avancée et un débit beaucoup plus élevé (~10x). Consultez la liste des modèles pris en charge ici : https://docs.vllm.ai/en/latest/models/supported_models.html\nIl suffit de remplacer le model_worker par le vllm_worker\npython3 -m fastchat.serve.vllm_worker --model-path lmsys/vicuna-7b-v1.5\n\n\n\nPour permettre le lancement et l’arrêt de modèles, et pour éviter qu’une erreur dans un des modèles ne déregle l’ensemble du système, une bonne pratique est souvent de conteneuriser les différentes parties. Cela necessite la préparation de quelques fichiers et quelques tests, mais ensuite, cela assure la reproductibilité de votre infrastructure. Une fois que les images sont préparées, on peut les arrêter, les relancer et les reproduire autant de fois que nécessaire.\nUne façon d’implémenter vos services avec FastChat est de faire :\n\nUn conteneur pour le controller\nUn conteneur pour l’API OpenAI like\nUn conteneur par modèle\n\nLes conteneurs pourront tous avoir la même image de base où l’on a installé les packages necessaires, comme vllm et notamment FastChat, que l’on a téléchargé et copié dans notre arbre local :\nDockerfile :\nFROM nvidia/cuda:12.2.0-devel-ubuntu20.04\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 python3.9-distutils curl\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copiez le répertoire FastChat dans le conteneur Docker\nCOPY ./FastChat_k /FastChat\n# COPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.4.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n# nvidia/cuda:12.2.0-runtime-ubuntu20.04 docker pull nvidia/cuda:12.2.0-devel-ubuntu20.04\nEnsuite, il faut lancer les conteneurs docker avec la bonne commande pour que chaque docker remplisse bien sa fonction. Cela se gère avec des docker_compose.yml\nLe fichier de déploiement des deux conteneurs obligatoires ressemblera à cela :\nversion: \"3.9\"\nservices:\n  fastchat-controller:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"21001:21001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n  fastchat-openai:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1,key2,key3\", \"--controller-address\", \"http://0.0.0.0:21001\"]\nEt le fichier de déploiement d’un modèle pourrait ressembler à ceci :\nversion: \"3.9\"\nservices:\n  fastchat-model-mixtral-latest:\n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - /data/models:/data/models\n      - ./FastChat:/FastChat  \n    environment:\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n      TRANSFORMERS_OFFLINE: 1\n    image: fastchat:cudadevel-latest \n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: [\"1\", \"5\"]\n              capabilities: [gpu]\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\",\n    \"--worker-address\", \"http://0.0.0.0:26003\", \"--host\", \"0.0.0.0\", \"--port\", \"26003\", \"--controller\", \"http://0.0.0.0:21001\", \n    \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"] #  \"--quantization\", \"awq\" \"--num-gpus\", \"2\"\nPour finir, il suffit de lancer les commandes associées à chaque docker_compose pour lancer tous les services. Par exemple,\n# Define your Docker Compose files\ncompose_openai_service=\"docker-compose_openai.yml\"\ncompose_mixtral=\"docker-compose_mistral.yml\"\n\n# Execute Docker Compose commands\necho \"Executing Docker Compose for $compose_openai_service\"\ndocker compose -f $compose_openai_service up -d\n\necho \"Executing Docker Compose for $compose_mixtral\"\ndocker compose -f $compose_mixtral up -d\nA ce stade, vous avez déjà une installation utilisable par plusieurs personnes (à condition que l’url soit accessible). Voici des exemples de code de cellules notebooks.\nimport openai\nimport requests\nimport json\n# to get proper authentication, make sure to use a valid key that's listed in\n# the --api-keys flag. if no flag value is provided, the `api_key` will be ignored.\nopenai.api_key = \"key1\" # 1rentrez l'api key\nopenai.api_base = \"http://10.156.254.10:8000/v1\" # mettre l'url du serveur\n#eventuellement régler des problèmes de proxy\n%env no_proxy=10.156.254.10,0.0.0.0\nmodels = openai.Model.list()\nfor d in models[\"data\"]:\n    print(d[\"id\"])\n# Instruct mode\nprompt = \"\"\"Bonjour toi. Donne moi un pays qui commence par F.\n\"\"\"\n\ncompletion = openai.Completion.create(\n    model=\"mixtral?\", \n    prompt=prompt, \n    max_tokens=25,\n    temperature=0.5,\n    top_p=1\n)\n# print the completion\nprint(completion.choices[0].text)",
    "crumbs": [
      "III-Deploiements",
      "Socle minimal"
    ]
  },
  {
    "objectID": "III-Deploiements/1_Socle_minimal.html#socle-minimal-pour-un-llm-camille-jérôme-conrad",
    "href": "III-Deploiements/1_Socle_minimal.html#socle-minimal-pour-un-llm-camille-jérôme-conrad",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Pour déployer un grand modèle de langage (LLM) dans une infrastructure, il est essentiel de comprendre comment requêter le modèle, les quelques couches techniques immédiates qui l’entourent et les solutions disponibles pour un déploiement efficace.\n\n\nLorsqu’il s’agit de mettre en service des applications basées sur des LLM, il y a 2 composants principaux : le moteur et le serveur. Le moteur gère tout ce qui concerne les modèles et le regroupement des demandes, tandis que le serveur gère l’acheminement des demandes des utilisateurs.\n\n\nLes moteurs sont les composants exécutant les modèles et tout ce que nous avons couvert jusqu’à présent sur le processus de génération avec différents types d’optimisations. À leur cœur, ce sont des bibliothèques Python. Ils gèrent le regroupement des demandes qui proviennent des utilisateurs vers notre chatbot et génèrent la réponse à ces demandes.\n\n\n\nLes serveurs sont responsables de l’orchestration des requêtes HTTP/gRPC entrantes des utilisateurs. Dans les applications du monde réel, nous aurons de nombreux utilisateurs qui posent des questions à notre chatbot à différents moments de la journée. Les serveurs mettent ces demandes en file d’attente et les transfèrent vers le moteur pour la génération de la réponse. Les serveurs apportent également les métriques telles que le débit et la latence, qui sont importantes à suivre pour le service de modèle.\n\n\n\nMoteurs\n\nOptimisation de la mémoire\nOptimisation spécifique au modèle\nPrise en charge du regroupement\n\nServeurs\n\nAPI HTTP/gRPC\nMise en file d’attente des demandes\nMise en service de plusieurs modèles\nPrise en charge de plusieurs moteurs\n\n\n\n\n\nQuels outils sont les mieux adaptés à nos besoins ? Comment choisir ? Voici un survol rapide de grands noms du milieu pour références.\n\nUne recommandation de framework rapide à prendre en main et dont l’utilité a déjà été prouvée dans une de nos administrations se trouve à la fin et est développée dans le prochain paragraphe.\n\nMoteurs\n\nTensorRT-LLM est une bibliothèque open-source qui optimise les performances d’inférence des grands modèles de langage (LLM) en utilisant les GPU NVIDIA Tensor Core. Elle utilise le parallélisme tensoriel, propose une API Python simple et comprend des versions optimisées de LLM populaires. Elle prend en charge le batching en vol et vise à simplifier la construction et l’expérimentation de nouveaux LLM. Cependant, les utilisateurs doivent spécifier la longueur d’entrée/sortie maximale et la taille de lot avant de construire le moteur, et la gestion de la mémoire du cache KV n’est pas open source.\nvLLM est une bibliothèque à hautes performances pour l’inférence et le service LLM, axée sur le débit de service et l’efficacité mémoire grâce à son mécanisme PagedAttention. Il prend en charge le batching continu, le parallélisme GPU et la sortie en streaming, ainsi que la compatibilité OpenAI. Cependant, la mémoire peut devenir un goulot d’étranglement avec des taux de demande élevés et de grandes tailles de lot.\n\nServeurs\n\nRayLLM avec RayServe est construit sur un framework de calcul distribué qui simplifie le développement et le déploiement de modèles d’IA à grande échelle. Il prend en charge les points de terminaison multi-modèles, les fonctionnalités serveur et les optimisations via les intégrations avec vLLM et TGI.\nTriton avec TensorRT-LLM fournit un logiciel d’inférence de serveur pour le déploiement et l’exécution efficaces de LLM avec des techniques telles que le batching en vol et le cache KV paginé.\n\nMoteurs et serveurs\n\nGénération de texte Inférence (TGI) est un serveur Rust, Python et gRPC utilisé chez HuggingFace pour HuggingChat, l’API d’inférence et le point de terminaison d’inférence. Il prend en charge le batching continu, le parallélisme tensoriel, la quantification, les mécanismes d’attention, le recuit simulé des logits et des LLM spécifiques. Cependant, la licence d’utilisation a été modifiée et n’est pas gratuite pour une utilisation commerciale.\nEnfin, Fastchat est une solution auto-hébergée pour héberger des modèles d’IA génératifs et qui propose la gestion des modèles, des API OpenAI-compatibles et une web interface simple.\n\n\nNous allons développer FastChat dans la partie suivante car c’est un outil qui a été testé et qui semble fournir beaucoup des éléments nécessaires pour une utilisation de première intention.\n\n\n\n\n\n\nPour certains cas d’usage, l’enjeu est de traiter de nombreuses données avec le même mode opératoire en un coup de manière ponctuelle. C’est ce qu’on appellera le traitement par batch. Cela consiste à charger un modèle, le requêter sur un tableau de prompt et obtenir la sortie pour pouvoir l’exporter. On peut le faire avec vLLM par exemple avec un morceau de code de ce type :\nfrom vllm import LLM, SamplingParams\nimport re\nimport pandas as pd\nimport re\nimport json\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n\nlist_data = json.load(open(\"Data.json\"))\n\nlist_prompts = [ v for x,v in list_data.items()]\nlist_ids = [ x for x,v in list_data.items() ]\n\nsampling_params = SamplingParams(temperature=0.1, top_p=0.1, max_tokens=4096)\nllm = LLM(model=\"/data/models/hub/models--upstage--Llama-2-70b-instruct-v2/snapshots/36b2a974642846b40fbbafaabad936cd6f8a7632\", tensor_parallel_size=2)\nprint(\"STARTING INFERENCE\")\noutputs = llm.generate(list_prompts, sampling_params)\n\nresume = { idx:output.outputs[0].text for idx, output in zip(list_ids, outputs) }\n\njson.dump(resume, open(\"Sortie.json\", \"w\"))\nMais cette méthodologie a des limites, car cela nécessite de bloquer des gpus, ce qui entraîne des problématiques de gestion et de partage.\n\n\n\nQue ce soit une équipe de plusieurs data-scientists, ou un ensemble d’application, si les besoins sont importants, les GPUs ont tout intérêt à être partagés. Il ne sera donc pas possible que chaque script python charge son modèle en mémoire et bloque des GPUs. Il est également plus rassurant de séparer l’infrastructure GPU des utilisateurs pour que chacun travaille dans son environnement, afin d’éviter les casses accidentelles.\nLa solution qui consiste à mettre à disposition des APIs vient répondre à ces problématiques. Les modèles sont cachés derrière les API, les datascientist et les applications peuvent venir les requêter et n’ont pas besoin de s’occuper de l’infrastructure. Ainsi, plutôt que chaque datascientist déploie un même modèle avec réservation de GPU, l’architecture en API permet la mise en commun du déploiement au même besoin.\n\nDans ce guide, FastChat est présenté comme exemple pour la simplicité mais d’autres solutions existent, avec chacunes leurs avantages et inconvénients.\n\n\n\n\n\nFastChat propose des API OpenAI-compatibles pour ses modèles pris en charge, de sorte que vous puissiez utiliser FastChat comme une alternative locale aux API OpenAI. Cela permet d’utiliser la bibliothèque openai-python et les commandes cURL, ce qui facilite le travail des datascientists.\nLa documentation complète est disponible sur le repo du module : https://github.com/lm-sys/FastChat/tree/main\nNous allons tout de même parcourir les grandes étapes pour pouvoir lancer son installation et ensuite l’utiliser.\n\n\nTout repose sur la complémentarité de trois services : le controller, les modèles et l’API. Il faut commencer par lancer le controller.\npython3 -m fastchat.serve.controller\nEnsuite, les model_workers. (Un modèle vicuna est pris pour l’exemple.)\npython3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5\nEt enfin, l’API.\npython3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n\n\n\nLe but de openai_api_server.py est d’implémenter un serveur d’API entièrement compatible avec OpenAI, de sorte que les modèles puissent être utilisés directement avec la bibliothèque openai-python.\nTout d’abord, installez le package Python OpenAI &gt;= 1.0 :\npip install --upgrade openai\nEnsuite, interagissez avec le modèle Vicuna :\nimport openai\n\nopenai.api_key = \"EMPTY\"\nopenai.base_url = \"http://localhost:8000/v1/\"\n\nmodel = \"vicuna-7b-v1.5\"\nprompt = \"Il était une fois\"\n\n# créer une complétion\ncompletion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)\n# imprimer la complétion\nprint(prompt + completion.choices[0].text)\n\n# créer une complétion de chat\ncompletion = openai.chat.completions.create(\n  model=model,\n  messages=[{\"role\": \"user\", \"content\": \"Bonjour ! Quel est votre nom ?\"}]\n)\n# imprimer la complétion\nprint(completion.choices[0].message.content)\n\n\n\ncURL est un autre bon outil pour observer la sortie de l’API.\nList Models:\ncurl http://localhost:8000/v1/models\nChat Completions:\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n  }'\nText Completions:\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"prompt\": \"Once upon a time\",\n    \"max_tokens\": 41,\n    \"temperature\": 0.5\n  }'\nEmbeddings:\ncurl http://localhost:8000/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"input\": \"Hello world!\"\n  }'\n\n\n\nVous pouvez utiliser vLLM comme une mise en œuvre optimisée d’un travailleur dans FastChat. Il offre une mise en batch continue avancée et un débit beaucoup plus élevé (~10x). Consultez la liste des modèles pris en charge ici : https://docs.vllm.ai/en/latest/models/supported_models.html\nIl suffit de remplacer le model_worker par le vllm_worker\npython3 -m fastchat.serve.vllm_worker --model-path lmsys/vicuna-7b-v1.5\n\n\n\nPour permettre le lancement et l’arrêt de modèles, et pour éviter qu’une erreur dans un des modèles ne déregle l’ensemble du système, une bonne pratique est souvent de conteneuriser les différentes parties. Cela necessite la préparation de quelques fichiers et quelques tests, mais ensuite, cela assure la reproductibilité de votre infrastructure. Une fois que les images sont préparées, on peut les arrêter, les relancer et les reproduire autant de fois que nécessaire.\nUne façon d’implémenter vos services avec FastChat est de faire :\n\nUn conteneur pour le controller\nUn conteneur pour l’API OpenAI like\nUn conteneur par modèle\n\nLes conteneurs pourront tous avoir la même image de base où l’on a installé les packages necessaires, comme vllm et notamment FastChat, que l’on a téléchargé et copié dans notre arbre local :\nDockerfile :\nFROM nvidia/cuda:12.2.0-devel-ubuntu20.04\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 python3.9-distutils curl\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copiez le répertoire FastChat dans le conteneur Docker\nCOPY ./FastChat_k /FastChat\n# COPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.4.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n# nvidia/cuda:12.2.0-runtime-ubuntu20.04 docker pull nvidia/cuda:12.2.0-devel-ubuntu20.04\nEnsuite, il faut lancer les conteneurs docker avec la bonne commande pour que chaque docker remplisse bien sa fonction. Cela se gère avec des docker_compose.yml\nLe fichier de déploiement des deux conteneurs obligatoires ressemblera à cela :\nversion: \"3.9\"\nservices:\n  fastchat-controller:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"21001:21001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n  fastchat-openai:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1,key2,key3\", \"--controller-address\", \"http://0.0.0.0:21001\"]\nEt le fichier de déploiement d’un modèle pourrait ressembler à ceci :\nversion: \"3.9\"\nservices:\n  fastchat-model-mixtral-latest:\n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - /data/models:/data/models\n      - ./FastChat:/FastChat  \n    environment:\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n      TRANSFORMERS_OFFLINE: 1\n    image: fastchat:cudadevel-latest \n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: [\"1\", \"5\"]\n              capabilities: [gpu]\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\",\n    \"--worker-address\", \"http://0.0.0.0:26003\", \"--host\", \"0.0.0.0\", \"--port\", \"26003\", \"--controller\", \"http://0.0.0.0:21001\", \n    \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"] #  \"--quantization\", \"awq\" \"--num-gpus\", \"2\"\nPour finir, il suffit de lancer les commandes associées à chaque docker_compose pour lancer tous les services. Par exemple,\n# Define your Docker Compose files\ncompose_openai_service=\"docker-compose_openai.yml\"\ncompose_mixtral=\"docker-compose_mistral.yml\"\n\n# Execute Docker Compose commands\necho \"Executing Docker Compose for $compose_openai_service\"\ndocker compose -f $compose_openai_service up -d\n\necho \"Executing Docker Compose for $compose_mixtral\"\ndocker compose -f $compose_mixtral up -d\nA ce stade, vous avez déjà une installation utilisable par plusieurs personnes (à condition que l’url soit accessible). Voici des exemples de code de cellules notebooks.\nimport openai\nimport requests\nimport json\n# to get proper authentication, make sure to use a valid key that's listed in\n# the --api-keys flag. if no flag value is provided, the `api_key` will be ignored.\nopenai.api_key = \"key1\" # 1rentrez l'api key\nopenai.api_base = \"http://10.156.254.10:8000/v1\" # mettre l'url du serveur\n#eventuellement régler des problèmes de proxy\n%env no_proxy=10.156.254.10,0.0.0.0\nmodels = openai.Model.list()\nfor d in models[\"data\"]:\n    print(d[\"id\"])\n# Instruct mode\nprompt = \"\"\"Bonjour toi. Donne moi un pays qui commence par F.\n\"\"\"\n\ncompletion = openai.Completion.create(\n    model=\"mixtral?\", \n    prompt=prompt, \n    max_tokens=25,\n    temperature=0.5,\n    top_p=1\n)\n# print the completion\nprint(completion.choices[0].text)",
    "crumbs": [
      "III-Deploiements",
      "Socle minimal"
    ]
  }
]